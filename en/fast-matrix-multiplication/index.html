<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Luc J. Bourhis Musings - Matrix Multiplication at Peak Performance</title>
<link href="../../article.css" rel="stylesheet">
<script src="../../js/jquery-3.3.1.min.js"></script>
<script src="../../js/jquery.color.plus-names-2.1.2.min.js"></script>
<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,400i,700,700i%7CPT+Sans:400,400i,700,700i" rel="stylesheet">
<meta content="nanoc 4.11.5" name="generator">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  // we do not need delimiters as kramdown will transform a $$ XXX $$
  // into span's or div's, the inner ones containing XXX,
  // and with the appropriate id's and classes for an accompanying
  // script to run Mathjax to render the content of those tags.
  tex2jax: {
    inlineMath: [],
    displayMath: [],
  },

  TeX: {
    // this is part of our handmade management of equation numbers
    equationNumbers: {
      autoNumber: "AMS"
    },

    // this is part of our handmade LaTeX macro management
    Macros: {
      reals: ["\\mathbb{R}"],
integers: ["\\mathbb{N}"],
Re: ["\\operatorname{Re}"],
Im: ["\\operatorname{Im}"],
set: ["\\left\\{\\left. #1 \\right| #2 \\right\\}", 2],
ket: ["\\left| #1 \\right\\rangle", 1],
bra: ["\\left\\langle #1 \\right|", 1],
braket: ["\\left\\langle #1 \\vphantom{#2}\\right|\\left.\\vphantom{#1} #2 \\right\\rangle", 2],
expect: ["\\left\\langle #1 \\right\\rangle", 1],
comm: ["\\left[ #1, #2 \\right]", 2],
matnorm: ["\\Vert #1 \\Vert", 1],
mathbbone: ["\\\\unicode{x1D7D9}"],
sign: ["\\operatorname{sign}"]
    }
  },

  menuSettings: {
    zoom: "Double-Click",
  },
});
// MathJax.Hub.Register.StartupHook("SVG Jax Ready",function () {
//   var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;
//   VARIANT["normal"].fonts.unshift("MathJax_SansSerif");
//   VARIANT["bold"].fonts.unshift("MathJax_SansSerif-bold");
//   VARIANT["italic"].fonts.unshift("MathJax_SansSerif-italic");
//   VARIANT["-tex-mathit"].fonts.unshift("MathJax_SansSerif-italic");
// });


</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_SVG"></script>

<script src="https://use.fontawesome.com/0e6d8b9bc7.js"></script>
</head>
<body>
<script>
  $(document).ready(function() {
    $("#topics-menu-toggle").click(function(event) {
      event.preventDefault();
      $("#topics-menu").toggleClass('is-open');
    });
  
    $(".bibliography-reference").click(function(event){
      var hash = this.href.substr(this.href.indexOf('#')+1);
      var duration = 1500;
      var target = $(
        "#" + hash.replace( /(:|\.|\[|\]|,|=|@)/g, "\\$1" ));
      target.animate({backgroundColor:"LightSkyBlue"}, duration/2)
      .animate({backgroundColor:"white"}, duration/2);
    })
  });
</script>
<nav class="menu" id="topics-menu">
<span class="menu-toggle" id="topics-menu-toggle">
<span class="fa fa-bars"></span>
<span class="toc_label">
Table of Contents
</span>
</span>
<div class="topics">
<hr>
<span class="topic_categories">
Home
<a class="flags" href="..">
<img src="../icon.png">
</a>
<a class="flags" href="../../fr">
<img src="../../fr/icon.png">
</a>
</span>
<div class="topic_categories">computing</div>
<ul class="topic_list">
<li class="titles"><a href="./">Matrix Multiplication at Peak Performance</a></li>
</ul>
<div class="topic_categories">physics</div>
<ul class="topic_list">
<li class="titles"><a href="../relativity-groups/">Change of inertial frames: what transforms are possible?</a></li>
</ul>
<hr>
</div>
</nav>
<div id="main">
<h1>
Matrix Multiplication at Peak Performance
</h1>
<div class="author">
Luc J. Bourhis
</div>
<div id="body">
<div class="abstract">
<p>High performance matrix multiplication is the workhorse which makes possible the whole field of high performance linear algebra. In this article, we succinctly motivate the importance of that field, and explain in simple terms why it must rely so much on fast matrix multiplication. Then we explore how to implement the latter with more technical details.</p>


</div>
<div id="just-after-abstract"></div>
<h2 id="introduction">Introduction</h2>

<p>This essay will discuss the principles underlying the coding of high performance matrix multiplication in libraries such as <abbr>ATLAS</abbr><sup id="fnref:3"><a href="#fn:3" class="footnote">1</a></sup> or <abbr>OpenBLAS</abbr>.<sup id="fnref:4"><a href="#fn:4" class="footnote">2</a></sup> Those provide implementation of the so-called <abbr>BLAS</abbr>, for Basic Linear Algebra System. This is a comprehensive set of the typical operations with which any algorithm manipulating matrices and vectors in a linear manner can be written, i.e. linear combinations, matrix-vector products, matrix-matrix products, etc. <abbr>BLAS</abbr> is designed to be efficient on single or multi-processor systems with relativity fast communication between the CPU’s and they are tuned to the class of processors which have dominated computing since the mid-90’s, which are characterised by a deep memory hierarchy, the famous L1, L2, L3, … caches.</p>

<p>The plan of this article is as follow. First we will explain why efficient matrix multiplication is of paramount importance for the whole of the field of computational linear algebra. Then we will explain the salient point of those “modern” architectures <abbr>BLAS</abbr> are optimised for.</p>

<h2 id="on-the-importance-of-matrix-multiplication">On the importance of matrix multiplication</h2>

<p>Solving systems of linear equations, computing the eigenvalues and the eigenvectors of a matrix, or solving a linear least-squares problem, to name only a few applications of linear algebra, are very common tasks in a wide range of fields. Huge matrices are not unusual (e.g. least-squares with about 100000 parameters and millions of observations) and therefore extremely fast algorithms are compulsory for many applications. It was discovered in the 80’s that the computer architectures emerging at the time, were most efficient at performing matrix multiplications. As a result, all classic linear algebra algorithms have been rewritten throughout the 90’s so as to perform nearly all floating point operations as part of matrix multiplications, and more generally of the so-called level 3 operations, which are defined as those such that the number of floating-point operations (flops) scales as a third-order polynomials of the matrix dimensions whereas the number of matrix elements that are touched scales as a second-order polynomials of those dimensions. As a result, the speed of modern algorithms such as those provided by LAPACK<sup id="fnref:2"><a href="#fn:2" class="footnote">3</a></sup> almost entirely hinges on the speed of those level 3 operations. They include solving triangular system of equations with multiple right-hand sides and rank-k updates (e.g. the symmetric one which reads <script type="math/tex">\begin{equation}
C \rightarrow C + \alpha A A^T
\end{equation}</script>), besides matrix multiplication.</p>

<p>The reason for the efficiency of level 3 operations is rooted in the memory hierarchy of modern architectures, which we are therefore going to discuss next.</p>

<h2 id="memory-hierarchy">Memory hierarchy</h2>

<p>Modern architectures are characterised by a strong memory hierarchy, whose key layers are sketched on the following diagram.</p>

<p class="width640px"><img src="memory-hierarchy.svg" alt="Memory hierarchy"></p>

<p>There is a huge speed difference between the different memory layers. For example, on “recent” x86 processors, it takes 1 cycle to add two floating point values stored in registers, about 4 cycles to load a value from L1 cache into registers, about 10 cycles to do so from L2 cache, from 40 to a few hundred cycles to load a value from L3 cache, and finally about 60 ns to get a value all the way from DRAM. Thus we should design our algorithms so that once a chunk of data has been moved upward through this memory hierarchy, as many floating point operations are performed on it that do not require further data motion.</p>

<p>Level 3 operations are an ideal case in that respect. Let us take the example of the matrix product <script type="math/tex">\begin{equation}
C = A B
\end{equation}</script> where all matrices are of dimension <script type="math/tex">\begin{equation}
n \times n
\end{equation}</script>, where <script type="math/tex">\begin{equation}
n
\end{equation}</script> is small enough that <script type="math/tex">\begin{equation}
A
\end{equation}</script>, <script type="math/tex">\begin{equation}
B
\end{equation}</script> and <script type="math/tex">\begin{equation}
C
\end{equation}</script> fit in the processor caches. By definition, for any <script type="math/tex">\begin{equation}
i=1,\cdots,n
\end{equation}</script> and any <script type="math/tex">\begin{equation}
j=1,\cdots,n
\end{equation}</script></p>

<script type="math/tex; mode=display">\begin{equation}
C_{ij} = \sum_{p=1}^n A_{ip} B_{pj}.
\end{equation}</script>

<p>This requires to move <script type="math/tex">\begin{equation}
O(n^2)
\end{equation}</script> matrix elements from RAM to the caches and it will then take <script type="math/tex">\begin{equation}
O(n^3)
\end{equation}</script> flops. Thus the cost of moving each matrix element is amortised over <script type="math/tex">\begin{equation}
O(n)
\end{equation}</script> flops. This property of matrix multiplication, and by extension all the other level 3 operations that share this <script type="math/tex">\begin{equation}
O(n^3)
\end{equation}</script> vs <script type="math/tex">\begin{equation}
O(n^2)
\end{equation}</script> property, is the key to their performance. Clearly, we should choose as big a value of n as possible to maximise the number of flops per memory operation, but under the constrain that all matrices fit in the caches.</p>

<p>But then, how does this apply to general matrices that are too big to fit in the caches? The trick is to slice them into blocks that do fit in those caches and to multiply those blocks. This is the subject of the next section.</p>

<h2 id="general-matrix-matrix-product-gemm">General matrix-matrix product (GEMM)</h2>

<p>The key will be to dice and slice matrices in the right manner and we will start with a picturesque description of this decomposition. We are going to follow as closely as possible the prescriptions presented in Goto<sup id="fnref:1"><a href="#fn:1" class="footnote">4</a></sup> that are at the core of Goto<abbr>BLAS</abbr>2, one of the fastest implementation of <abbr>BLAS</abbr> (this project is now continued under the name of <abbr>OpenBLAS</abbr><sup id="fnref:4:1"><a href="#fn:4" class="footnote">2</a></sup>).</p>

<p>Given a matrix <script type="math/tex">\begin{equation}
A
\end{equation}</script> of dimension <script type="math/tex">\begin{equation}
m \times k
\end{equation}</script>, a matrix <script type="math/tex">\begin{equation}
B
\end{equation}</script> of dimension <script type="math/tex">\begin{equation}
k \times n
\end{equation}</script> and a matrix <script type="math/tex">\begin{equation}
C
\end{equation}</script> of dimension <script type="math/tex">\begin{equation}
m \times n
\end{equation}</script>, we are interested in the operation <script type="math/tex">\begin{equation}
C = C + A B
\end{equation}</script>. A fast implementation is obtained by a three-layered decomposition into blocks and panels.</p>

<p>The first layer is a decomposition of <script type="math/tex">\begin{equation}
A
\end{equation}</script> into vertical panels and of <script type="math/tex">\begin{equation}
B
\end{equation}</script> into horizontal ones:</p>

<p class="width640px"><img src="AB-blocking-1.svg" alt="Matrix multiplication blocking (1)"></p>

<p>The rightmost block of <script type="math/tex">\begin{equation}
A
\end{equation}</script> (resp. the bottommost block of <script type="math/tex">\begin{equation}
B
\end{equation}</script>) may have less than <script type="math/tex">\begin{equation}
k_c
\end{equation}</script> columns (resp. rows). Each product <script type="math/tex">\begin{equation}
A_p B_p
\end{equation}</script> is called a general panel-panel product (GEPP) by Goto et al. For matrices with large <script type="math/tex">\begin{equation}
m
\end{equation}</script> or large <script type="math/tex">\begin{equation}
n
\end{equation}</script>, either of these panels may not fit in the L1/L2 caches. As a result, one relies on a second layer of decompositions: each <script type="math/tex">\begin{equation}
A_p
\end{equation}</script> is cut into blocks. The resulting decomposition of GEPP can then be represented as follow.</p>

<p class="width640px"><img src="AB-blocking-2.svg" alt="Matrix multiplication blocking (2)"></p>

<p>Again the bottommost block has likely a dimension smaller than <script type="math/tex">\begin{equation}
m_c
\end{equation}</script>. The general block-panel product <script type="math/tex">\begin{equation}
A_{ip} B_p
\end{equation}</script> is called GEBP by Goto et al. All the floating point operations are performed in GEBP. This is therefore the only place to optimise to the fullest. It is especially essential to choose dimensions <script type="math/tex">\begin{equation}
k_c
\end{equation}</script> and <script type="math/tex">\begin{equation}
m_c
\end{equation}</script> as large as possible to amortise the memory operations but not so large that those blocks and panels would trash the L1/L2 caches and the TLB (c.f. Appendices). Specifically, Goto and van de Geijn advocates that the following properties should hold:</p>

<ol>
  <li>TODO</li>
</ol>

<p>The values <script type="math/tex">\begin{equation}
k_c
\end{equation}</script> = 256 and <script type="math/tex">\begin{equation}
m_c
\end{equation}</script> = 512 fit well with a wide variety of processors, especially the most common x86_64 varieties, as listed in Goto<sup id="fnref:1:1"><a href="#fn:1" class="footnote">4</a></sup> and in the source code of <abbr>OpenBLAS</abbr>. They are however by no means optimal. If one want the utmost performance, one should adapt those values to each architecture by solving the constraints described above, at least approximately.</p>

<p>Eventually, the operations shall be performed on the CPU registers. The third and last layer of blocks is designed to use the registers as efficiently as possible.<br>
The left-hand side is split into thin slices of rows whereas of the right-hand side is split into thin slices of columns, resulting in the decomposition of GEBP into a grid of tiny block, each being the product of two thin panels:</p>

<p class="width640px"><img src="AB-blocking-3.svg" alt="Matrix multiplication blocking (3)"></p>

<p>The values of <script type="math/tex">\begin{equation}
m_r
\end{equation}</script> and <script type="math/tex">\begin{equation}
n_r
\end{equation}</script> are chosen so that each tiny block of the product can be held entirely in CPU registers. The only memory operations involved in the computation of each of these tiny block consist in loading the elements of the two thin panels into the registers. Moreover the loops over the <script type="math/tex">\begin{equation}
m_r
\end{equation}</script> rows of the left-hand side and over the <script type="math/tex">\begin{equation}
n_r
\end{equation}</script> columns of the right-hand side can be entirely unrolled because <script type="math/tex">\begin{equation}
m_r
\end{equation}</script> and <script type="math/tex">\begin{equation}
n_r
\end{equation}</script> are very small. This leaves only one loop over <script type="math/tex">\begin{equation}
k_c
\end{equation}</script> columns of the left-hand side and <script type="math/tex">\begin{equation}
k_c
\end{equation}</script> rows of the right-hand side during the execution of which the elements of the tiny block of <script type="math/tex">\begin{equation}
C
\end{equation}</script> are updated entirely inside registers. When this loop terminates, and only then, the corresponding block of <script type="math/tex">\begin{equation}
C
\end{equation}</script> is updated in RAM. <script type="math/tex">\begin{equation}
m_r
\end{equation}</script> and <script type="math/tex">\begin{equation}
n_r
\end{equation}</script> are highly dependent on the number of registers and the size of vector units (the SIMD type commonly found on modern processors<sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup>) featured by the processor. For a purely scalar processor, <script type="math/tex">\begin{equation}
m_r
\end{equation}</script> = 2 and <script type="math/tex">\begin{equation}
n_r
\end{equation}</script> = 4 usually provide a bottom line.</p>

<h2 id="appendix-tlb">Appendix: TLB</h2>

<p>The role of L1/L2/L3 caches and of virtual memory is mundane knowledge but we believe it not to be the case for the TLB. It is however crucial to correctly determine the constrains on the block size above. We will therefore dedicate the rest of this section to it. First, we shall remember that physical addresses in RAM and virtual addresses on disk are mapped onto each other by the <em>page table</em>, which does also keep track of whether a memory page is loaded in RAM, or swapped to the disk. In order to speed up the retrieval of pages from the disk, most architectures cache the information about the most recently used pages in a so-called Translation Look-aside Buffer (TLB). It roughly works as follow. When a line of L2 cache is to be updated with data starting at a given address in memory, the system looks up for that address in the TLB:</p>

<ul>
  <li>if it is there, then it proceeds with the requested memory operation: loading the appropriate memory from the disk if it is not already in RAM, and then moving data from RAM to L2 cache;</li>
  <li>if not, (so-called TLB miss), the address is searched in the page table; when it is found, the TLB is updated, and then the memory operation takes place as in the previous case.</li>
</ul>

<p>Contrary to a L1/L2 cache miss that can be prevented by prefetching data, a TLB miss always stalls the CPU because of the TLB update and it should therefore be avoided at all cost by working with chunks of data that fit inside the TLB address space.</p>

<h2 id="bibliography">Bibliography</h2>

<div class="footnotes">
  <ol>
    <li id="fn:3">
      <p>Automatically Tuned Linear Algebra Software (<a href="http://math-atlas.sourceforge.net"><abbr>ATLAS</abbr></a>) <a href="#fnref:3" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:4">
      <p><a href="http://www.openblas.net"><abbr>OpenBLAS</abbr></a> <a href="#fnref:4" class="reversefootnote">↩</a> <a href="#fnref:4:1" class="reversefootnote">↩<sup>2</sup></a></p>
    </li>
    <li id="fn:2">
      <p>LAPACK Users’ Guide.
  E Anderson, Z Bai, Christian H. Bischof, S Blackford, J Demmel,
  J Dongarra, J Du Croz, A Greenbaum, S Hammarling, A McKenney,
  and D Sorensen.
  Society for Industrial and Applied Mathematics,
  Philadelphia, PA, Third. <a href="#fnref:2" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:1">
      <p>Kazushige Goto and Robert A. van de Geijn,
  Anatomy of high-performance matrix multiplication,
  ACM Transactions on Mathematical Software 34 (2008), no. 3, 12:1–25. <a href="#fnref:1" class="reversefootnote">↩</a> <a href="#fnref:1:1" class="reversefootnote">↩<sup>2</sup></a></p>
    </li>
    <li id="fn:5">
      <p>SSE2/3/4 on x86 and x86_64, NEON on ARM, Altivec on PowerPC and Cell, VIS on SPARC, just to name a few popular processors, <a href="#fnref:5" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

</div>

<div id="license">
<div>
<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="license">
<img alt="Creative Commons License" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png">
</a>
</div>
<div>
This work is licensed under a
<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="license">
Creative Commons Attribution-NonCommercial-ShareAlike 4.0
International License.
</a>
</div>
</div>

</div>

</body>
</html>
